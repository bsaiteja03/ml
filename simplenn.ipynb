{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"simplenn.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"dCHMVGm-zl2o","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9f4eda6c-9bb6-4f62-e6b0-803f16d0352d","executionInfo":{"status":"ok","timestamp":1576467903661,"user_tz":-330,"elapsed":6997,"user":{"displayName":"sai kiran","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAGLRN68if71sEBxO2HfhRl8Keah1uPZIyaDPyaTAU=s64","userId":"05122384550265843126"}}},"source":["pip install graphviz"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (0.10.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HEMBibf3zuO6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"927062b2-65db-44d7-d181-c2e8457f123e","executionInfo":{"status":"ok","timestamp":1576467917763,"user_tz":-330,"elapsed":7151,"user":{"displayName":"sai kiran","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAGLRN68if71sEBxO2HfhRl8Keah1uPZIyaDPyaTAU=s64","userId":"05122384550265843126"}}},"source":["import numpy as np\n","import tensorflow as tf\n","input_size = 2 \n","hidden_size = 20 \n","batch_size = 100\n","iter_number = 5000\n","learning_rate = 1e-1\n","\n","inputs = tf.placeholder(tf.float32, [None, input_size])\n","targets = tf.placeholder(tf.float32, [None, 2])\n","\n","W1 = tf.Variable(tf.random_normal([input_size, hidden_size], stddev=0.01)) # input to hidden\n","W2 = tf.Variable(tf.random_normal([hidden_size, 2], stddev=0.01)) # hidden to output\n","b1 = tf.Variable(tf.zeros([hidden_size])) # hidden bias\n","b2 = tf.Variable(tf.zeros([2])) # output bias\n","\n","z1 = tf.matmul(inputs, W1) + b1\n","h = tf.nn.sigmoid(z1)\n","z2 = tf.matmul(h, W2) + b2\n","y = tf.nn.softmax(z2)\n","\n","cross_entropy = tf.reduce_mean(-tf.reduce_sum(targets * tf.log(y), reduction_indices=[1]))\n","train_step = tf.train.AdagradOptimizer(learning_rate, initial_accumulator_value=1e-8).minimize(cross_entropy)\n","correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(targets, 1))\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","\n","sess = tf.InteractiveSession()\n","tf.global_variables_initializer().run()\n","for i in range(iter_number):\n","\tinputs_batch = (np.random.rand(batch_size, input_size)*4-2).astype('float32')\n","\ttargets_batch = np.zeros([batch_size,2]).astype('float32')\n","\tmask = (np.linalg.norm(inputs_batch, axis=1)<1).astype(int)\n","\tfor t in range(batch_size):\n","\t\ttargets_batch[t][mask[t]] = 1 # one-hot encoding\n","\t# run training step\n","\t_, loss, acc = sess.run([train_step, cross_entropy, accuracy], feed_dict={inputs: inputs_batch, targets: targets_batch})\n","\t# print progress\n","\tif i % 50 == 0: print('iter {}, loss: {:.4f}, accuracy: {:.2f}'.format(i, loss, acc)) "],"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","iter 0, loss: 0.6978, accuracy: 0.17\n","iter 50, loss: 0.4004, accuracy: 0.88\n","iter 100, loss: 0.5893, accuracy: 0.73\n","iter 150, loss: 0.4838, accuracy: 0.81\n","iter 200, loss: 0.4917, accuracy: 0.80\n","iter 250, loss: 0.4367, accuracy: 0.83\n","iter 300, loss: 0.5588, accuracy: 0.70\n","iter 350, loss: 0.3196, accuracy: 0.87\n","iter 400, loss: 0.3437, accuracy: 0.80\n","iter 450, loss: 0.3028, accuracy: 0.81\n","iter 500, loss: 0.2329, accuracy: 0.90\n","iter 550, loss: 0.2577, accuracy: 0.88\n","iter 600, loss: 0.1699, accuracy: 0.98\n","iter 650, loss: 0.1469, accuracy: 0.98\n","iter 700, loss: 0.1621, accuracy: 0.94\n","iter 750, loss: 0.1280, accuracy: 0.98\n","iter 800, loss: 0.0899, accuracy: 0.99\n","iter 850, loss: 0.1022, accuracy: 0.99\n","iter 900, loss: 0.1140, accuracy: 0.99\n","iter 950, loss: 0.0714, accuracy: 1.00\n","iter 1000, loss: 0.1369, accuracy: 0.95\n","iter 1050, loss: 0.0818, accuracy: 0.98\n","iter 1100, loss: 0.0919, accuracy: 0.98\n","iter 1150, loss: 0.0994, accuracy: 0.98\n","iter 1200, loss: 0.1192, accuracy: 0.95\n","iter 1250, loss: 0.0945, accuracy: 0.97\n","iter 1300, loss: 0.0793, accuracy: 1.00\n","iter 1350, loss: 0.0832, accuracy: 0.99\n","iter 1400, loss: 0.0692, accuracy: 1.00\n","iter 1450, loss: 0.0671, accuracy: 0.99\n","iter 1500, loss: 0.0702, accuracy: 0.98\n","iter 1550, loss: 0.1004, accuracy: 0.97\n","iter 1600, loss: 0.0701, accuracy: 0.99\n","iter 1650, loss: 0.0715, accuracy: 1.00\n","iter 1700, loss: 0.0753, accuracy: 0.97\n","iter 1750, loss: 0.0846, accuracy: 0.98\n","iter 1800, loss: 0.0831, accuracy: 0.97\n","iter 1850, loss: 0.0321, accuracy: 1.00\n","iter 1900, loss: 0.0538, accuracy: 1.00\n","iter 1950, loss: 0.0654, accuracy: 0.99\n","iter 2000, loss: 0.0633, accuracy: 1.00\n","iter 2050, loss: 0.0536, accuracy: 0.99\n","iter 2100, loss: 0.0547, accuracy: 0.99\n","iter 2150, loss: 0.0529, accuracy: 0.99\n","iter 2200, loss: 0.0650, accuracy: 0.99\n","iter 2250, loss: 0.0364, accuracy: 0.99\n","iter 2300, loss: 0.0505, accuracy: 1.00\n","iter 2350, loss: 0.0467, accuracy: 1.00\n","iter 2400, loss: 0.0623, accuracy: 0.99\n","iter 2450, loss: 0.0641, accuracy: 0.99\n","iter 2500, loss: 0.0680, accuracy: 0.99\n","iter 2550, loss: 0.0459, accuracy: 1.00\n","iter 2600, loss: 0.0682, accuracy: 0.97\n","iter 2650, loss: 0.0589, accuracy: 1.00\n","iter 2700, loss: 0.0470, accuracy: 1.00\n","iter 2750, loss: 0.0403, accuracy: 0.99\n","iter 2800, loss: 0.0399, accuracy: 1.00\n","iter 2850, loss: 0.0287, accuracy: 1.00\n","iter 2900, loss: 0.0665, accuracy: 0.98\n","iter 2950, loss: 0.0462, accuracy: 1.00\n","iter 3000, loss: 0.0293, accuracy: 1.00\n","iter 3050, loss: 0.0439, accuracy: 1.00\n","iter 3100, loss: 0.0564, accuracy: 0.98\n","iter 3150, loss: 0.0707, accuracy: 1.00\n","iter 3200, loss: 0.0309, accuracy: 0.99\n","iter 3250, loss: 0.0408, accuracy: 1.00\n","iter 3300, loss: 0.0419, accuracy: 0.99\n","iter 3350, loss: 0.0493, accuracy: 1.00\n","iter 3400, loss: 0.0274, accuracy: 1.00\n","iter 3450, loss: 0.0548, accuracy: 1.00\n","iter 3500, loss: 0.0434, accuracy: 1.00\n","iter 3550, loss: 0.0329, accuracy: 1.00\n","iter 3600, loss: 0.0382, accuracy: 1.00\n","iter 3650, loss: 0.0460, accuracy: 1.00\n","iter 3700, loss: 0.0610, accuracy: 0.99\n","iter 3750, loss: 0.0322, accuracy: 1.00\n","iter 3800, loss: 0.0405, accuracy: 1.00\n","iter 3850, loss: 0.0223, accuracy: 1.00\n","iter 3900, loss: 0.0540, accuracy: 1.00\n","iter 3950, loss: 0.0490, accuracy: 0.99\n","iter 4000, loss: 0.0570, accuracy: 0.99\n","iter 4050, loss: 0.0245, accuracy: 0.99\n","iter 4100, loss: 0.0447, accuracy: 0.99\n","iter 4150, loss: 0.0124, accuracy: 1.00\n","iter 4200, loss: 0.0274, accuracy: 1.00\n","iter 4250, loss: 0.0368, accuracy: 1.00\n","iter 4300, loss: 0.0417, accuracy: 1.00\n","iter 4350, loss: 0.0233, accuracy: 1.00\n","iter 4400, loss: 0.0425, accuracy: 1.00\n","iter 4450, loss: 0.0312, accuracy: 1.00\n","iter 4500, loss: 0.0429, accuracy: 0.99\n","iter 4550, loss: 0.0489, accuracy: 0.99\n","iter 4600, loss: 0.0334, accuracy: 1.00\n","iter 4650, loss: 0.0434, accuracy: 1.00\n","iter 4700, loss: 0.0547, accuracy: 0.99\n","iter 4750, loss: 0.0402, accuracy: 1.00\n","iter 4800, loss: 0.0514, accuracy: 1.00\n","iter 4850, loss: 0.0335, accuracy: 1.00\n","iter 4900, loss: 0.0389, accuracy: 1.00\n","iter 4950, loss: 0.0399, accuracy: 0.99\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0yPjhOP-z072","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}